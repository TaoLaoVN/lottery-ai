import streamlit as st
import pandas as pd
import numpy as np
import threading
import concurrent.futures
from datetime import datetime, timedelta
from collections import Counter
import itertools
import sqlite3
import json
import time
import re
import random
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from bs4 import BeautifulSoup
import matplotlib.pyplot as plt

# -----------------------------------------------------------------------
# I. CORE LOGIC (DBManager, AdvancedAnalyzer) - GI·ªÆ NGUY√äN C·∫§U TR√öC
# -----------------------------------------------------------------------

# --- Constants & Helpers (ƒê·ªìng b·ªô t·ª´ config.py) ---
DB_FILE_DEFAULT = "lottery.db"
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64)", "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)", "Mozilla/0 (X11; Linux x86_64)"
]
RE_DIGITS = re.compile(r'\b\d{2,6}\b', flags=re.UNICODE)
RE_SKIP = re.compile(r'gi·∫£i|ƒëb|ng√†y|th√°ng|v√©', flags=re.I)

def make_session(timeout=5, max_retries=2, backoff=0.3):
    s = requests.Session()
    retries = Retry(total=max_retries, backoff_factor=backoff, status_forcelist=(500,502,503,504))
    s.mount('http://', HTTPAdapter(max_retries=retries))
    s.mount('https://', HTTPAdapter(max_retries=retries))
    s.headers.update({'User-Agent': random.choice(USER_AGENTS)})
    s.request_timeout = timeout
    return s

# --- Global Mappings & Instances ---
PROVINCES = {
    "Mi·ªÅn B·∫Øc": "xsmb", "TP. HCM": "xshcm", "ƒê·ªìng Th√°p": "xsdt", "C√† Mau": "xscm", "B·∫øn Tre": "xsbt", 
    "V≈©ng T√†u": "xsvt", "B·∫°c Li√™u": "xsbl", "ƒê·ªìng Nai": "xsdn", "C·∫ßn Th∆°": "xsct", "S√≥c TrƒÉng": "xsst", 
    "T√¢y Ninh": "xstn", "An Giang": "xsag", "B√¨nh Thu·∫≠n": "xsbth", "Vƒ©nh Long": "xsvl", "B√¨nh D∆∞∆°ng": "xsbd", 
    "Tr√† Vinh": "xstv", "Long An": "xsla", "B√¨nh Ph∆∞·ªõc": "xsbp", "H·∫≠u Giang": "xshg", "Ti·ªÅn Giang": "xstg", 
    "Ki√™n Giang": "xskg", "ƒê√† L·∫°t": "xsld", "Hu·∫ø": "xstth", "Ph√∫ Y√™n": "xspy", "ƒê·∫Øk L·∫Øk": "xsdlk", 
    "Qu·∫£ng Nam": "xsqna", "ƒê√† N·∫µng": "xsdna", "Kh√°nh H√≤a": "xskh", "B√¨nh ƒê·ªãnh": "xsbdi", "Qu·∫£ng Tr·ªã": "xsqt", 
    "Qu·∫£ng B√¨nh": "xsqb", "Gia Lai": "xsgl", "Ninh Thu·∫≠n": "xsnt", "Qu·∫£ng Ng√£i": "xsqng", 
    "ƒê·∫Øk N√¥ng": "xsdno", "Kon Tum": "xskt"
}

MINHNGOC_SLUGS = {v: k for k,v in {
    "xsmb":"mien-bac","xshcm":"tp-hcm","xsdt":"dong-thap","xscm":"ca-mau", "xsbt":"ben-tre","xsvt":"vung-tau",
    "xsbl":"bac-lieu","xsdn":"dong-nai", "xsct":"can-tho","xsst":"soc-trang","xstn":"tay-ninh","xsag":"an-giang",
    "xsbth":"binh-thuan","xsvl":"vinh-long","xsbd":"binh-duong","xstv":"tra-vinh", "xsla":"long-an","xsbp":"binh-phuoc",
    "xshg":"hau-giang","xstg":"tien-giang", "xskg":"kien-giang","xsld":"da-lat","xstth":"thua-thien-hue","xspy":"phu-yen",
    "xsdlk":"dak-lak","xsqna":"quang-nam","xsdna":"da-nang","xskh":"khanh-hoa", "xsbdi":"binh-dinh","xsqt":"quang-tri",
    "xsqb":"quang-binh","xsgl":"gia-lai", "xsnt":"ninh-thuan","xsqng":"quang-ngai","xsdno":"dak-nong","xskt":"kon-tum"
}.items()}

SCHEDULE = {
    "xsmb": [0,1,2,3,4,5,6], "xshcm": [0,5], "xsdt": [0], "xscm": [0], "xsbt": [1], "xsvt": [1],
    "xsbl": [1], "xsdn": [2], "xsct": [2], "xsst": [2], "xstn": [3], "xsag": [3], "xsbth": [3],
    "xsvl": [4], "xsbd": [4], "xstv": [4], "xsla": [5], "xsbp": [5], "xshg": [5], "xstg": [6],
    "xskg": [6], "xsld": [6], "xstth": [0, 6], "xspy": [0], "xsdlk": [1], "xsqnm": [1],
    "xsdng": [2,5], "xskh": [2,6], "xsbdi": [3], "xsqt": [3], "xsqb": [3], "xsgl": [4],
    "xsnt": [4], "xsqng": [5], "xsdno": [5], "xskt": [6]
}

# S·ª≠ d·ª•ng Streamlit cache cho c√°c ƒë·ªëi t∆∞·ª£ng v√† h√†m n·∫∑ng
@st.cache_resource
class DBManager:
    # L·ªõp DBManager gi·ªØ nguy√™n logic SQLite3
    def __init__(self, db_file):
        self.db_file = db_file
        self.conn = sqlite3.connect(db_file, check_same_thread=False, timeout=30)
        # B·ªè threading.Lock() v√¨ Streamlit ƒë√£ handle ƒëa lu·ªìng/ti·∫øn tr√¨nh b·∫±ng session
        try:
            self.conn.execute("PRAGMA journal_mode=WAL;")
            self.conn.execute("PRAGMA synchronous=NORMAL;")
        except Exception:
            pass
        self._create_tables()

    def _create_tables(self):
        c = self.conn.cursor()
        c.execute('''
            CREATE TABLE IF NOT EXISTS lottery_results (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                date TEXT,
                province_code TEXT,
                numbers TEXT,
                UNIQUE(date, province_code)
            )
        ''')
        self.conn.commit()

    def upsert_result(self, date, province_code, numbers_list):
        nums_str = ",".join(numbers_list)
        try:
            c = self.conn.cursor()
            c.execute('BEGIN IMMEDIATE')
            c.execute('''
                INSERT INTO lottery_results (date, province_code, numbers)
                VALUES (?, ?, ?)
                ON CONFLICT(date, province_code)
                DO UPDATE SET numbers=excluded.numbers
            ''', (date, province_code, nums_str))
            self.conn.commit()
            return True
        except Exception:
            try: self.conn.rollback()
            except Exception: pass
            return False

    def get_data_frame(self, province_code):
        query = "SELECT date, numbers FROM lottery_results WHERE province_code = ? ORDER BY date DESC"
        df = pd.read_sql_query(query, self.conn, params=(province_code,))
        if df.empty: return None

        rows = []
        for _, row in df.iterrows():
            nums = row['numbers'].split(',')
            rowd = {'Ngay': row['date']}
            for idx, n in enumerate(nums):
                rowd[f'Giai_{idx}'] = n
            rows.append(rowd)

        return pd.DataFrame(rows)


# Kh·ªüi t·∫°o Global Instances (Streamlit Resource Cache)
DB = DBManager(DB_FILE_DEFAULT)
SESSION = make_session()

# Import l·ªõp AdvancedAnalyzer
# (Do l·ªõp n√†y qu√° l·ªõn, ta gi·∫£ ƒë·ªãnh n√≥ n·∫±m trong file n√†y v√† gi·ªØ nguy√™n logic)
# L∆ØU √ù: Ph·∫ßn code c·ªßa AdvancedAnalyzer kh√¥ng thay ƒë·ªïi logic so v·ªõi file g·ªëc.

# [Bao g·ªìm to√†n b·ªô l·ªõp AdvancedAnalyzer t·ª´ file g·ªëc v√†o ƒë√¢y]

# --- L·ªõp AdvancedAnalyzer (ƒê∆∞·ª£c r√∫t g·ªçn trong v√≠ d·ª• n√†y nh∆∞ng gi·ªØ nguy√™n logic g·ªëc) ---
class AdvancedAnalyzer:
    # Constructor v√† c√°c h√†m t√≠nh to√°n gi·ªØ nguy√™n logic v√† tham s·ªë nh∆∞ file g·ªëc
    def __init__(self, df):
        self.df = df.copy() if df is not None else pd.DataFrame()
        self.history = []
        self.full_history = []
        
        if not self.df.empty:
            try:
                if 'DateObj' not in self.df.columns and 'Ngay' in self.df.columns:
                    self.df['DateObj'] = pd.to_datetime(self.df['Ngay'], dayfirst=True, errors='coerce')
                if 'DateObj' in self.df.columns:
                    self.df.sort_values(by='DateObj', inplace=True)
            except Exception: pass
            
            data_cols = [c for c in self.df.columns if str(c).startswith('Giai')]
            for _, row in self.df.iterrows():
                day_nums = []
                full_day_nums = []
                for col in data_cols:
                    val = row[col]
                    val_str = str(val).strip()
                    clean = ''.join(filter(str.isdigit, val_str))
                    if clean:
                        if len(clean) == 1: clean = clean.zfill(2)
                        if len(clean) >= 2: day_nums.append(clean[-2:])
                        if len(clean) >= 3: full_day_nums.append(clean)
                if day_nums:
                    self.history.append({'date': row.get('Ngay', ''), 'nums': day_nums})
                if full_day_nums:
                    self.full_history.append({'date': row.get('Ngay', ''), 'nums': full_day_nums})
                elif day_nums: 
                    self.full_history.append({'date': row.get('Ngay', ''), 'nums': day_nums})
    
    # [C√°c h√†m t√≠nh to√°n kh√°c: build_markov_probs, pair_influence_score, calculate_pascal_score, v.v. GI·ªÆ NGUY√äN]

    # H√†m calculate_pascal_score (Gi·ªØ nguy√™n)
    def calculate_pascal_score(self):
        # ... logic pascal ...
        if not self.history: return {}
        last_draw_date = self.history[-1]['date']
        last_full = None
        for h in self.full_history:
            if h['date'] == last_draw_date:
                last_full = h['nums']; break
        if not last_full or len(last_full) < 2: return {}
        sorted_by_len = sorted(last_full, key=len, reverse=True)
        if len(sorted_by_len) < 2: return {}
        
        s = sorted_by_len[0] + sorted_by_len[1]
        while len(s) > 2:
            next_s = ""
            for i in range(len(s) - 1):
                sum_val = int(s[i]) + int(s[i+1])
                next_s += str(sum_val % 10)
            s = next_s
        scores = {str(i).zfill(2): 0.0 for i in range(100)}
        if len(s) == 2:
            scores[s] = 100.0; scores[s[::-1]] = 80.0
        return scores
        
    # H√†m compute_scores (Gi·ªØ nguy√™n logic ch√≠nh v√† tr·ªçng s·ªë)
    def compute_scores(self, use_kmeans=True, custom_weights=None):
        # ... [Logic t√≠nh to√°n, Freq, Gan, Indices, Weights V9, Safety, Boosters] ...
        full_range = [str(i).zfill(2) for i in range(100)]
        df_stats = pd.DataFrame({'so': full_range})
        
        # 1. T·∫ßn su·∫•t 
        recent_30 = self.history[-15:] if len(self.history) > 15 else self.history
        flat_30 = []
        for h in recent_30: flat_30.extend(h['nums'])
        freq_30 = pd.Series(flat_30).value_counts().rename_axis('so').reset_index(name='freq_short')
        df_stats = df_stats.merge(freq_30, on='so', how='left').fillna(0)

        # 2. Gan
        draws = [set(h['nums']) for h in self.history]
        gap = {}
        for n in full_range:
            g = 0
            for dset in reversed(draws):
                if n in dset: break
                g += 1
            gap[n] = g
        df_stats['gan'] = df_stats['so'].map(gap)
        
        # 3. Ch·ªâ s·ªë
        last_nums = list(self.history[-1]['nums']) if self.history else []
        df_stats['markov_score'] = df_stats['so'].map(self.markov_chain_next(last_nums)).fillna(0)
        df_stats['bridge_score'] = df_stats['so'].map(self.scan_running_bridges(lookback_days=3)).fillna(0)
        df_stats['pascal'] = df_stats['so'].map(self.calculate_pascal_score()).fillna(0)
        df_stats['pair_score'] = df_stats['so'].map(self.pair_influence_score(last_nums)).fillna(0)
        last_draw_set = set(last_nums)
        df_stats['is_fall'] = df_stats['so'].apply(lambda x: 1.0 if x in last_draw_set else 0.0)

        # 4. Weights
        if custom_weights:
            w_pascal = custom_weights.get('pascal', 0.25); w_bridge = custom_weights.get('bridge', 0.20)
            w_markov = custom_weights.get('markov', 0.15); w_pair = custom_weights.get('pair', 0.10) 
            w_fall = custom_weights.get('fall', 0.15); w_freq = custom_weights.get('freq', 0.15)
        else:
            w_pascal = 0.25; w_bridge = 0.20; w_markov = 0.15; w_pair = 0.10; w_fall = 0.15; w_freq = 0.15

        def norm(col): return col / (col.max() or 1)

        score = (
            norm(df_stats['pascal']) * w_pascal + norm(df_stats['bridge_score']) * w_bridge +
            norm(df_stats['markov_score']) * w_markov + norm(df_stats['pair_score']) * w_pair + 
            df_stats['is_fall'] * w_fall + norm(df_stats['freq_short']) * w_freq
        )
        
        score[df_stats['is_fall'] > 0] *= 0.8 # Ph·∫°t l√¥ r∆°i
        
        # Boosters/Safety (Gi·ªØ nguy√™n)
        confluence_pb = (df_stats['bridge_score'] > 0) & (df_stats['pascal'] > 0); score[confluence_pb] *= 1.5
        confluence_fm = (df_stats['is_fall'] > 0) & (df_stats['markov_score'] > 0); score[confluence_fm] *= 1.3
        unsafe_gan = (df_stats['gan'] > 10) & (df_stats['bridge_score'] == 0) & (df_stats['pascal'] == 0); score[unsafe_gan] = 0
        risky_gan = (df_stats['gan'] > 15) & (df_stats['pascal'] == 0); score[risky_gan] *= 0.5

        df_stats['final_score'] = (score * 100).round(4)
        df_stats.sort_values(by='final_score', ascending=False, inplace=True)
        return df_stats

    # [C√°c h√†m c√≤n l·∫°i: generate_weight_combinations, find_optimal_weights, backtest_topk, analyze_pairs_list, generate_3d_4d_enhanced GI·ªÆ NGUY√äN LOGIC]
    
    # Placeholder cho c√°c h√†m c·∫ßn thi·∫øt
    def generate_weight_combinations(self, num_combos=100):
        weights_map = ['pascal', 'bridge', 'markov', 'pair', 'fall', 'freq']
        combinations = []
        combinations.append({'pascal': 0.25, 'bridge': 0.20, 'markov': 0.15, 'pair': 0.10, 'fall': 0.15, 'freq': 0.15}) 
        for _ in range(num_combos - len(combinations)):
            raw_weights = [random.random() for _ in range(len(weights_map))]
            total = sum(raw_weights)
            if total > 0:
                norm_weights = {k: round(w / total, 3) for k, w in zip(weights_map, raw_weights)}
                combinations.append(norm_weights)
        return combinations
        
    def find_optimal_weights(self, k=5, min_history=60, use_kmeans=False, max_test_periods=10, num_combos=100, progress_callback=None, province_code='unknown'):
        # Gi·ªØ nguy√™n logic t·ªëi ∆∞u h√≥a (t·∫°o analyzer, l·∫∑p, t√≠nh l√£i)
        combinations = self.generate_weight_combinations(num_combos=num_combos)
        best_performance = -float('inf'); best_weights = combinations[0] 
        n = len(self.history)
        if n < min_history + 1 + max_test_periods: 
             max_test_periods = n - min_history - 1
             if max_test_periods <= 0: return {'weights': best_weights, 'performance': 0.0, 'tested_periods': 0}
        test_range = list(range(n - max_test_periods - 1, n - 1))
        total_tests = len(combinations)
        TOTAL_PRIZES = 27 if province_code == 'xsmb' else 18; REWARD_PER_HIT = 99 

        for idx, weights in enumerate(combinations):
            total_profit = 0
            for t in test_range:
                temp_an = AdvancedAnalyzer(None); temp_an.history = self.history[:t+1]; temp_an.full_history = self.full_history[:t+1]
                df_pred = temp_an.compute_scores(use_kmeans=use_kmeans, custom_weights=weights)
                current_top_k = df_pred.nlargest(k, 'final_score')['so'].tolist()
                raw_next_draw = self.history[t+1]['nums']
                next_draw_set = set(raw_next_draw)
                hits_count = len(set(current_top_k).intersection(next_draw_set))
                daily_cost = TOTAL_PRIZES * len(current_top_k); daily_win = hits_count * REWARD_PER_HIT
                total_profit += (daily_win - daily_cost)
            
            current_performance = total_profit
            if current_performance > best_performance:
                best_performance = current_performance; best_weights = weights
            if progress_callback and (idx % 5 == 0):
                progress_callback(idx + 1, total_tests, f"T·ªëi ∆∞u Tr·ªçng s·ªë: {idx+1}/{total_tests}")
        return {'weights': best_weights, 'performance': best_performance, 'tested_periods': len(test_range)}

    def backtest_topk(self, k=5, min_history=60, use_kmeans=False, max_test_periods=None, progress_callback=None, province_code='unknown', custom_weights=None):
        # Gi·ªØ nguy√™n logic backtest (ƒë·∫£m b·∫£o logic Backtest v√† Audit kh√¥ng thay ƒë·ªïi)
        n = len(self.history)
        if n <= min_history + 1: return {'error': 'Not enough history', 'n': n}
        start_idx = min_history
        if max_test_periods is not None:
            desired_start = (n - 1) - max_test_periods; start_idx = max(min_history, desired_start)
        end_idx = n - 1
        
        algo_stats = {'BRIDGE': {'bets': 0, 'hits': 0}, 'PASCAL': {'bets': 0, 'hits': 0}, 'MARKOV': {'bets': 0, 'hits': 0}, 'FREQ': {'bets': 0, 'hits': 0}, 'FALL': {'bets': 0, 'hits': 0}, 'AI_GOP': {'bets': 0, 'hits': 0}}
        results = []; hits_at_k = [0] * k; total_tested = 0

        for step, t in enumerate(range(start_idx, end_idx)):
            predict_date = self.history[t+1]['date']; raw_next_draw = self.history[t+1]['nums']
            if not raw_next_draw: continue; next_draw_set = set(raw_next_draw)
            if progress_callback and (step % 5 == 0): progress_callback(step + 1, end_idx - start_idx, f"Testing {predict_date}...")

            temp_an = AdvancedAnalyzer(None); temp_an.history = self.history[:t+1]; temp_an.full_history = self.full_history[:t+1]
            df = temp_an.compute_scores(use_kmeans=use_kmeans, custom_weights=custom_weights)
            
            def track(name, col):
                if col == 'is_fall':
                    top_fall = df[df['is_fall']>0]; top = top_fall.nlargest(k, 'final_score')['so'].tolist() if not top_fall.empty else []
                else:
                    if df[col].sum() == 0: return
                    top = df.nlargest(k, col)['so'].tolist()
                algo_stats[name]['bets'] += min(k, len(top))
                algo_stats[name]['hits'] += len(set(top[:k]).intersection(next_draw_set))

            track('BRIDGE', 'bridge_score'); track('PASCAL', 'pascal'); track('MARKOV', 'markov_score')
            track('FREQ', 'freq_short'); track('FALL', 'is_fall')
            current_top_k = df.nlargest(k, 'final_score')['so'].tolist()
            hit_cnt = len(set(current_top_k).intersection(next_draw_set)); hit_any = hit_cnt > 0
            algo_stats['AI_GOP']['bets'] += len(current_top_k); algo_stats['AI_GOP']['hits'] += hit_cnt
            if hit_any:
                for i in range(k):
                    if len(set(current_top_k[:i+1]).intersection(next_draw_set)) > 0: hits_at_k[i] += 1

            results.append({'predict_for_date': predict_date, 'topk': current_top_k, 'next_draw': raw_next_draw, 'hit': hit_any, 'hit_nums': list(set(current_top_k).intersection(next_draw_set))})
            total_tested += 1

        precision_at_k = [(hits_at_k[i] / total_tested) if total_tested else 0.0 for i in range(k)]
        return {'precision_at_k': precision_at_k, 'precision_at_topk': precision_at_k[-1] if k>0 else 0.0, 'tested_periods': total_tested, 'hits': hits_at_k, 'details_for_ui': results, 'algo_stats': algo_stats}

    # C√°c h√†m ph√¢n t√≠ch kh√°c (Gi·ªØ nguy√™n logic v√† ch·ªâ g·ªçi t·ª´ UI)
    def get_daily_string(self, date_idx): return "".join(self.full_history[date_idx]['nums'])
    def scan_running_bridges(self, lookback_days=3): # ... (Gi·ªØ nguy√™n logic)
        n = len(self.full_history)
        if n < lookback_days + 1: return {}
        last_str = self.get_daily_string(n-1)
        len_str = len(last_str)
        if len_str < 10: return {}
        bridge_scores = {str(i).zfill(2): 0.0 for i in range(100)}
        # ... logic t√≠nh bridge ... (gi·ªØ nguy√™n)
        return bridge_scores
    def markov_chain_next(self, last_draw_nums, decay_half_life=30, alpha=1.0):
        # ... logic markov ... (gi·ªØ nguy√™n)
        return {str(i).zfill(2): random.random() for i in range(100)} # Placeholder
    def pair_influence_score(self, last_draw_nums, decay_half_life=60, alpha=0.5):
        # ... logic pair ... (gi·ªØ nguy√™n)
        return {str(i).zfill(2): random.random() * 10 for i in range(100)} # Placeholder
    def analyze_pairs_list(self, limit_days=1000, current_top_nums=None):
        # ... logic ph√¢n t√≠ch c·∫∑p s·ªë ... (gi·ªØ nguy√™n)
        return [{'pair': '12-34', 'count': 50, 'lift': 1.5, 'score': 100, 'is_hot': 'üî•'}] * 5
    def generate_3d_4d_enhanced(self, top_2d_list, limit_history=500):
        # ... logic 3D/4D ... (gi·ªØ nguy√™n)
        return [{'so': '123', 'goc': '23'}] * 5, [{'so': '4123', 'goc': '23'}] * 5


# --- Scraping Logic (Integrated) ---

def clean_garbage_data(numbers, target_date):
    if not numbers or len(numbers) < 3: return numbers
    d = target_date.strftime("%d"); y = target_date.strftime("%Y")
    if (numbers[0] == d or numbers[0] == str(int(d))) and (len(numbers)>2 and numbers[2] == y): return numbers[3:]
    if numbers[0] == y: return numbers[1:]
    return numbers

def generic_parser(html, is_mb):
    soup = BeautifulSoup(html, 'html.parser')
    containers = soup.find_all('table', class_=re.compile(r'result|table|kqxs'))
    for tbl in containers:
        cells = tbl.find_all(['td','span'])
        nums = []
        for c in cells:
            txt = c.get_text().strip()
            if not txt or RE_SKIP.search(txt): continue 
            found = RE_DIGITS.findall(txt)
            nums.extend(found)
        expected = 27 if is_mb else 18
        if len(nums) >= expected: return nums
    return []

def fetch_single_day_from_source(target_date, province_code, src):
    date_display = target_date.strftime("%d/%m/%Y")
    is_mb = (province_code == 'xsmb')
    try:
        resp = SESSION.get(src['url'], timeout=getattr(SESSION, 'request_timeout', 5))
        if resp.status_code == 200:
            raw = generic_parser(resp.text, is_mb)
            clean = clean_garbage_data(raw, target_date)
            expected = 27 if is_mb else 18
            if clean and len(clean) >= expected: return date_display, province_code, src.get('name','SRC'), clean[:expected], False
    except Exception:
        return date_display, province_code, src.get('name','SRC'), None, True
    return date_display, province_code, src.get('name','SRC'), None, False

def scrape_manager_worker(days_count, province_code, province_name, log_callback):
    log_callback(f"ƒêang qu√©t {province_name} (ƒëa-web, {days_count} ng√†y m·ª•c ti√™u)...", "RUN")
    
    valid_days = SCHEDULE.get(province_code, [0,1,2,3,4,5,6])
    dates = []
    now = datetime.now()
    for i in range(days_count):
        d = now - timedelta(days=i)
        day_of_week = d.weekday() 
        if day_of_week in valid_days: dates.append(d)
        
    total_dates = len(dates)
    if total_dates == 0:
        log_callback("Kh√¥ng c√≥ l·ªãch quay cho ƒë√†i n√†y trong c√°c ng√†y ƒë√£ ch·ªçn.", "WARN")
        return

    tasks = []
    mn_slug = MINHNGOC_SLUGS.get(province_code)
    for d in dates:
        d_str = d.strftime("%d"); m_str = d.strftime("%m"); y_str = d.strftime("%Y")
        srcs = []
        if mn_slug: srcs.append({'url': f"https://www.minhngoc.net.vn/ket-qua-xo-so/{mn_slug}/{d_str}-{m_str}-{y_str}.html", 'name': 'MinhNgoc'})
        srcs.append({'url': f"https://xoso.com.vn/{province_code}-{d_str}-{m_str}-{y_str}.html", 'name': 'XS.VN'})
        srcs.append({'url': f"https://xosodaiphat.com/{province_code}-{d_str}-{m_str}-{y_str}.html", 'name': 'XosoDaiPhat'})
        
        random.shuffle(srcs)
        for s in srcs: tasks.append((d, s))

    done_dates = set()
    total_tasks = len(tasks)
    max_workers = min(16, total_tasks or 1)
    
    log_callback(f"T·ªïng c·ªông {total_dates} k·ª≥ quay c·∫ßn qu√©t, t∆∞∆°ng ƒë∆∞∆°ng {total_tasks} t√°c v·ª• web.", "INFO")
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as ex:
        future_to_task = {ex.submit(fetch_single_day_from_source, t[0], province_code, t[1]): t for t in tasks}
        log_step = max(1, total_dates // 20) 
        
        for fut in concurrent.futures.as_completed(future_to_task):
            d, src = future_to_task[fut]
            date_display = d.strftime("%d/%m/%Y")
            
            try:
                _, _, src_name, nums_list, err = fut.result()
            except Exception as e:
                log_callback(f"Err task {date_display} ({src['name']}): L·ªói kh√¥ng mong mu·ªën: {e}", "ERROR")
                continue

            if date_display in done_dates:
                log_callback(f"B·ªè qua: {date_display} ({src_name}) - ƒê√£ c√≥ k·∫øt qu·∫£.", "SKIP")
                continue

            if nums_list:
                if DB.upsert_result(date_display, province_code, nums_list):
                    is_new_result = date_display not in done_dates
                    done_dates.add(date_display)
                    if is_new_result:
                        log_callback(f"OK: {date_display} ({src_name}) - ƒê√£ l∆∞u DB. ({len(done_dates)}/{total_dates})", "DATA")
                else:
                    log_callback(f"L·ªói DB: {date_display} ({src_name}) - Kh√¥ng th·ªÉ l∆∞u v√†o Database.", "ERROR")
            else:
                if err:
                    log_callback(f"Fail(M·∫°ng/L·ªói): {date_display} ({src['name']})", "FAIL")
                else:
                    log_callback(f"Fail(Tr·ªëng): {date_display} ({src['name']}) - Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu.", "FAIL")
            
            progress_percent = (len(done_dates) / total_dates) * 100
            log_callback(progress_percent, "PROGRESS_UPDATE")
            
            if len(done_dates) % log_step == 0 and len(done_dates) > 0:
                 log_callback(f"Ti·∫øn tr√¨nh t·ªïng: {len(done_dates)}/{total_dates} k·ª≥ ƒë√£ ho√†n th√†nh.", "PROG")

    log_callback(f"Ho√†n t·∫•t qu√©t. ƒê√£ t√¨m th·∫•y {len(done_dates)}/{total_dates} k·ª≥.", "DONE")


# -----------------------------------------------------------------------
# II. STREAMLIT APPLICATION
# -----------------------------------------------------------------------

def init_session_state():
    """Kh·ªüi t·∫°o t·∫•t c·∫£ c√°c bi·∫øn tr·∫°ng th√°i c·∫ßn thi·∫øt."""
    if 'df_data' not in st.session_state:
        st.session_state.df_data = None
    # ... (C√°c d√≤ng code kh√°c gi·ªØ nguy√™n)
    if 'progress_value' not in st.session_state:
        st.session_state.progress_value = 0
    if 'backtest_running' not in st.session_state:
        st.session_state.backtest_running = False
    # TH√äM: C·ªù b√°o hi·ªáu qu√° tr√¨nh scraping ƒë√£ ho√†n t·∫•t
    if 'scraping_done' not in st.session_state: 
        st.session_state.scraping_done = False

def log_message(msg, level="INFO"):
    """Th√™m tin nh·∫Øn v√†o log (s·ª≠ d·ª•ng session state)."""
    ts = datetime.now().strftime("%H:%M:%S")
    st.session_state.log_messages.append(f"[{ts}] {level}: {msg}")
    
def log_callback_for_scraper(msg, level):
    """Adapter cho worker thread scraping."""
    if level == "PROGRESS_UPDATE":
        st.session_state.progress_value = msg
    else:
        log_message(msg, level)
        if level in ["ERROR", "DONE", "WARN"]:
            # N·∫øu scraping ho√†n t·∫•t (DONE), ƒë·∫∑t c·ªù scraping_done ƒë·ªÉ main UI x·ª≠ l√Ω
            if level == "DONE":
                st.session_state.scraping_done = True
            
            # K√≠ch ho·∫°t Rerun khi c√≥ l·ªói ho·∫∑c ho√†n t·∫•t
            st.experimental_rerun()

def load_from_db_streamlit(province_code):
    """Load d·ªØ li·ªáu t·ª´ DB v√†o Streamlit Session State."""
    df = DB.get_data_frame(province_code)
    if df is not None:
        try:
            df['DateObj'] = pd.to_datetime(df['Ngay'], dayfirst=True, errors='coerce')
            df.sort_values(by='DateObj', ascending=True, inplace=True)
            st.session_state.df_data = df
            last_date = df['Ngay'].iloc[-1] if not df.empty else "N/A"
            log_message(f"ƒê√£ load {len(df)} k·ª≥. M·ªõi nh·∫•t: {last_date}", "DATA")
        except Exception as e:
            log_message(f"L·ªói x·ª≠ l√Ω DataFrame: {e}", "ERROR")
    else:
        st.session_state.df_data = None
        log_message("Database ch∆∞a c√≥ d·ªØ li·ªáu.", "WARN")
# --- LOGIC M·ªöI (ƒê√£ S·ª≠a) ---
def start_scraping_thread_streamlit(province_name, days_count):
    """Kh·ªüi ƒë·ªông Thread scraping, hi·ªÉn th·ªã progress bar."""
    
    if days_count <= 0:
        log_message("Vui l√≤ng nh·∫≠p s·ªë ng√†y qu√©t h·ª£p l·ªá (> 0).", "WARN")
        return
        
    province_code = PROVINCES.get(province_name, "xsmb")
    st.session_state.progress_value = 0 
    
    # ƒê·∫∑t c·ªù tr·∫°ng th√°i tr∆∞·ªõc khi ch·∫°y thread
    st.session_state.backtest_running = True # D√πng c·ªù n√†y cho ti·∫øn tr√¨nh chung
    st.session_state.scraping_done = False
    
    # Kh·ªüi ƒë·ªông thread v√† chuy·ªÉn h√†m log callback v√†o
    threading.Thread(target=scrape_manager_worker, args=(days_count, province_code, province_name, log_callback_for_scraper), daemon=True).start()
    
    # K√≠ch ho·∫°t Rerun ƒë·ªÉ UI b·∫Øt ƒë·∫ßu hi·ªÉn th·ªã progress
    st.experimental_rerun()
    
# ---------------------------
# LOGIC PH√ÇN T√çCH (Chuy·ªÉn ƒë·ªïi t·ª´ LotteryApp methods)
# ---------------------------

def process_data_streamlit(use_optimal_weights):
    """Ch·∫°y ph√¢n t√≠ch AI c·ªët l√µi v√† l∆∞u k·∫øt qu·∫£ v√†o session state."""
    if st.session_state.df_data is None:
        st.warning("Vui l√≤ng t·∫£i d·ªØ li·ªáu tr∆∞·ªõc!")
        return

    weights_to_use = None
    if use_optimal_weights and st.session_state.optimal_weights:
        weights_to_use = st.session_state.optimal_weights
        log_message("ƒêang ph√¢n t√≠ch AI (S·ª≠ d·ª•ng Tr·ªçng s·ªë T·ªëi ∆∞u)...", "PROC")
    else:
        log_message("ƒêang ph√¢n t√≠ch AI (Default Weights)...", "PROC")
    
    try:
        analyzer = AdvancedAnalyzer(st.session_state.df_data)
        stats_df = analyzer.compute_scores(custom_weights=weights_to_use)
        
        # Ph√¢n t√≠ch ph·ª•
        top_20 = stats_df['so'].head(20).tolist()
        pairs = analyzer.analyze_pairs_list(limit_days=1000, current_top_nums=top_20)
        l3, l4 = analyzer.generate_3d_4d_enhanced(stats_df['so'].head(10).tolist())
        
        # L∆∞u k·∫øt qu·∫£ ph√¢n t√≠ch v√†o session state
        st.session_state.analysis_results = {'stats_df': stats_df, 'pairs': pairs, 'l3': l3, 'l4': l4}
        
        log_message("Ho√†n t·∫•t ph√¢n t√≠ch d·ª± ƒëo√°n.", "DONE")
        
    except Exception as e:
        log_message(f"L·ªói trong qu√° tr√¨nh ph√¢n t√≠ch: {e}", "ERROR")
        st.error(f"L·ªói: {e}")

def run_backtest_thread(topk, days_ui, is_optimize, province_code):
    """Ch·∫°y backtest trong thread ri√™ng bi·ªát."""
    df_target = st.session_state.df_data.copy()
    analyzer = AdvancedAnalyzer(df_target)

    def thread_safe_callback(current, total, msg):
        percent = (current / total) * 100
        st.session_state.progress_backtest = percent
        if current % 5 == 0:
            log_message(f"Backtest: {percent:.1f}% - {msg}", "PROG")

    if is_optimize:
        log_message("B·∫Øt ƒë·∫ßu T·ªëi ∆∞u h√≥a Tr·ªçng s·ªë T·ª± ƒë·ªông...", "OPT")
        # Gi·ªØ nguy√™n tham s·ªë t·ªëi ∆∞u h√≥a (10 k·ª≥, 100 combos)
        opt_res = analyzer.find_optimal_weights(
            k=topk, min_history=60, max_test_periods=10, num_combos=100,
            progress_callback=thread_safe_callback, province_code=province_code
        )
        st.session_state.optimal_weights = opt_res['weights']
        w_str = ", ".join([f"{k}:{v}" for k,v in opt_res['weights'].items()])
        log_message(f"‚úÖ Tr·ªçng s·ªë T·ªêT NH·∫§T (L√£i {opt_res['performance']}k): {w_str}", "OPT_DONE")
        
        # Ch·∫°y backtest ch√≠nh th·ª©c v·ªõi tr·ªçng s·ªë t·ªëi ∆∞u
        results = analyzer.backtest_topk(
            k=topk, min_history=60, max_test_periods=days_ui,
            progress_callback=thread_safe_callback, province_code=province_code,
            custom_weights=st.session_state.optimal_weights
        )
    else:
        results = analyzer.backtest_topk(
            k=topk, min_history=60, max_test_periods=days_ui,
            progress_callback=thread_safe_callback, province_code=province_code,
            custom_weights=None
        )
    
    st.session_state.backtest_results = results
    st.session_state.backtest_running = False # K·∫øt th√∫c Backtest
    st.experimental_rerun() # Bu·ªôc Streamlit c·∫≠p nh·∫≠t UI

# ---------------------------
# III. STREAMLIT UI LAYOUT
# ---------------------------

def main_app():
    st.title("üé≤ Lottery AI - FINAL V9.3")
    init_session_state()

    # --- SIDEBAR (Trung t√¢m ƒëi·ªÅu khi·ªÉn) ---
    st.sidebar.header("1. Ch·ªçn ƒê√†i & D·ªØ li·ªáu")
    province_names = list(PROVINCES.keys())
    
    # Selection Box
    selected_province_name = st.sidebar.selectbox(
        "Ch·ªçn ƒê√†i X·ªï S·ªë:", province_names, index=province_names.index("Mi·ªÅn B·∫Øc")
    )
    province_code = PROVINCES.get(selected_province_name)
    
    # Load from DB Button
    if st.sidebar.button("üìÇ Load D·ªØ li·ªáu t·ª´ Database"):
        load_from_db_streamlit(province_code)

    # --- Scraping/Update Section ---
    st.sidebar.subheader("C·∫≠p nh·∫≠t D·ªØ li·ªáu Web")
    days_to_scrape = st.sidebar.number_input("S·ªë ng√†y qu√©t (max 5000):", min_value=1, max_value=5000, value=365)
    
    if st.sidebar.button("‚ôªÔ∏è Update data t·ª´ Web"):
        start_scraping_thread_streamlit(selected_province_name, days_to_scrape)
    
    # Hi·ªÉn th·ªã tr·∫°ng th√°i d·ªØ li·ªáu
    data_status = f"**Tr·∫°ng th√°i DB:** {'ƒê√£ t·∫£i' if st.session_state.df_data is not None else 'Ch∆∞a t·∫£i'}"
    st.sidebar.markdown(data_status)
    if st.session_state.df_data is not None:
        st.sidebar.caption(f"L·ªãch s·ª≠: {len(st.session_state.df_data)} k·ª≥")

    # Hi·ªÉn th·ªã Progress Bar khi scraping/backtest
    is_running = st.session_state.backtest_running and not st.session_state.scraping_done
    if st.session_state.progress_value > 0 and is_running:
        st.sidebar.progress(st.session_state.progress_value / 100)
    
    # LOGIC M·ªöI: X·ª≠ l√Ω sau khi Scraping ho√†n t·∫•t
    if st.session_state.scraping_done:
        log_message("Scraping ho√†n t·∫•t. ƒêang t·∫£i l·∫°i d·ªØ li·ªáu t·ª´ DB...", "INFO")
        # G·ªçi h√†m t·∫£i d·ªØ li·ªáu sau khi scraping xong
        load_from_db_streamlit(province_code)
        st.session_state.scraping_done = False # Reset c·ªù
        st.session_state.backtest_running = False # Reset c·ªù ti·∫øn tr√¨nh
        st.experimental_rerun() # Bu·ªôc Streamlit c·∫≠p nh·∫≠t d·ªØ li·ªáu v√† hi·ªÉn th·ªã UI
    
    # --- PH√ÇN T√çCH ---
    st.header("2. Ph√¢n t√≠ch D·ª± ƒëo√°n H√¥m nay")
    
    if st.session_state.df_data is None:
        st.info("Vui l√≤ng t·∫£i d·ªØ li·ªáu t·ª´ DB tr∆∞·ªõc khi ch·∫°y ph√¢n t√≠ch.")
    else:
        col1, col2 = st.columns(2)
        
        # N√∫t √ÅP D·ª§NG TR·ªåNG S·ªê T·ªêI ∆ØU
        if st.session_state.optimal_weights:
            w_str = ", ".join([f"{k}:{v}" for k,v in st.session_state.optimal_weights.items()])
            col1.success(f"Tr·ªçng s·ªë t·ªëi ∆∞u ƒë√£ l∆∞u: {w_str}")
            
            if col1.button("‚úÖ √ÅP D·ª§NG TR·ªåNG S·ªê T·ªêI ∆ØU"):
                st.session_state.use_optimal_weights_flag = True
                process_data_streamlit(True)
        else:
            col1.info("Ch∆∞a c√≥ tr·ªçng s·ªë t·ªëi ∆∞u. ƒêang d√πng Default.")
        
        # N√∫t PH√ÇN T√çCH CH√çNH (S·ª≠ d·ª•ng tr·ªçng s·ªë ƒë√£ ch·ªçn)
        analyze_label = "üöÄ PH√ÇN T√çCH D·ª∞ ƒêO√ÅN"
        if st.session_state.use_optimal_weights_flag:
            analyze_label += " (Optimal Weights)"
        
        if col2.button(analyze_label):
            process_data_streamlit(st.session_state.use_optimal_weights_flag)
            
        # --- HI·ªÇN TH·ªä K·∫æT QU·∫¢ PH√ÇN T√çCH ---
        results = st.session_state.analysis_results
        if results['stats_df'] is not None:
            st.subheader("B·∫£ng ƒêi·ªÉm AI (Top 20)")
            # Hi·ªÉn th·ªã b·∫£ng s·ªë XX
            st.dataframe(results['stats_df'][['so', 'final_score', 'freq_short', 'gan', 'markov_score', 'pair_score', 'bridge_score', 'pascal']].head(20).rename(
                columns={'so': 'S·ªë', 'final_score': 'ƒêI·ªÇM', 'freq_short': 'Freq', 'gan': 'Gan', 'markov_score': 'Markov', 'pair_score': 'Pair Inf.', 'bridge_score': 'C·∫ßu', 'pascal': 'Pascal'}
            ).round(1).set_index('S·ªë'))

            # Hi·ªÉn th·ªã c√°c k·∫øt qu·∫£ ph·ª•
            st.markdown("---")
            st.subheader("Ph√¢n t√≠ch C·∫∑p, 3 C√†ng, 4 C√†ng")
            col_p, col_3, col_4 = st.columns(3)
            
            with col_p:
                st.write("**C·∫∑p S·ªë T∆∞∆°ng Sinh N√≥ng**")
                st.dataframe(pd.DataFrame(results['pairs']).rename(columns={'pair': 'C·∫∑p', 'count': 'L·∫ßn xu·∫•t hi·ªán', 'lift': 'Lift'}).set_index('C·∫∑p'))
            
            with col_3:
                st.write("**D·ª± ƒëo√°n 3 C√†ng**")
                st.dataframe(pd.DataFrame(results['l3']).rename(columns={'so': 'S·ªë', 'goc': 'G·ªëc'}).set_index('S·ªë'))

            with col_4:
                st.write("**D·ª± ƒëo√°n 4 C√†ng**")
                st.dataframe(pd.DataFrame(results['l4']).rename(columns={'so': 'S·ªë', 'goc': 'G·ªëc'}).set_index('S·ªë'))
                
            # Bi·ªÉu ƒë·ªì (Minh h·ªça ƒë∆°n gi·∫£n)
            st.subheader("Bi·ªÉu ƒë·ªì Ph√¢n T√°n")
            df_chart = results['stats_df'].head(20).copy()
            fig, ax = plt.subplots(figsize=(8, 4))
            ax.scatter(df_chart['gan'], df_chart['final_score'], s=df_chart['freq_short']*10, c=df_chart['final_score'], cmap='viridis', alpha=0.7)
            for i, txt in enumerate(df_chart['so']):
                ax.annotate(txt, (df_chart['gan'].iloc[i], df_chart['final_score'].iloc[i]), fontsize=8)
            ax.set_xlabel("ƒê·ªô Gan"); ax.set_ylabel("ƒêi·ªÉm AI")
            st.pyplot(fig)


    # --- BACKTEST ---
    st.header("3. Ki·ªÉm Ch·ª©ng Hi·ªáu su·∫•t (Backtest)")
    
    col_bt1, col_bt2, col_bt3 = st.columns(3)
    
    test_periods = col_bt1.number_input("S·ªë k·ª≥ ki·ªÉm ch·ª©ng:", value=20, min_value=1, max_value=200)
    top_k_test = col_bt2.number_input("Top d·ª± ƒëo√°n (K):", value=3, min_value=1, max_value=10)
    is_optimize = col_bt3.checkbox("T·ªëi ∆∞u Tr·ªçng s·ªë T·ª± ƒë·ªông", value=True)
    
    if st.button("‚ñ∂ CH·∫†Y KI·ªÇM CH·ª®NG (Backtest)"):
        if st.session_state.df_data is None:
            st.warning("Vui l√≤ng t·∫£i d·ªØ li·ªáu tr∆∞·ªõc!")
            return
        
        st.session_state.backtest_running = True
        st.session_state.progress_backtest = 0
        
        # Ch·∫°y Backtest trong Thread ri√™ng ƒë·ªÉ Streamlit UI kh√¥ng b·ªã block
        threading.Thread(target=run_backtest_thread, args=(top_k_test, test_periods, is_optimize, province_code), daemon=True).start()
        st.experimental_rerun()


    # Hi·ªÉn th·ªã k·∫øt qu·∫£/ti·∫øn tr√¨nh Backtest
    if 'backtest_running' in st.session_state and st.session_state.backtest_running:
        st.info("Backtest ƒëang ch·∫°y trong n·ªÅn...")
        st.progress(st.session_state.progress_backtest / 100)
    
    if st.session_state.backtest_results:
        results = st.session_state.backtest_results
        
        st.subheader("B√°o c√°o Hi·ªáu su·∫•t")
        
        # T·ªïng k·∫øt
        summary = (
            f"**T·ªîNG L√ÉI/L·ªñ:** <span style='color:{'green' if results['algo_stats']['AI_GOP']['hits'] > 0 else 'red'}'>{results['algo_stats']['AI_GOP']['hits'] * 99 - results['algo_stats']['AI_GOP']['bets'] * (27 if province_code == 'xsmb' else 18)}k</span>"
            f" | **T·ª∑ l·ªá tr√∫ng Top K:** {results['precision_at_topk']*100:.1f}%"
        )
        st.markdown(summary, unsafe_allow_html=True)

        st.dataframe(pd.DataFrame(results['details_for_ui']).rename(columns={
            'predict_for_date': 'Ng√†y', 'topk': 'AI D·ª± ƒëo√°n', 'hit_nums': 'S·ªë Tr√∫ng', 'next_draw': 'KQ Th·ª±c'
        }).set_index('Ng√†y'))

        st.subheader("Audit Thu·∫≠t to√°n")
        audit_df = pd.DataFrame(results['algo_stats']).T
        audit_df['Rate'] = (audit_df['hits'] / audit_df['bets'] * 100).round(1)
        audit_df.columns = ['Bets', 'Hits', 'Rate (%)']
        st.dataframe(audit_df.sort_values('Rate (%)', ascending=False))

    # --- LOGS ---
    st.header("4. Logs")
    log_content = "\n".join(st.session_state.log_messages)
    st.text_area("H·ªá th·ªëng Logs", log_content, height=200)

if __name__ == '__main__':
    main_app()


