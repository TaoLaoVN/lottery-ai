import streamlit as st
import pandas as pd
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import threading
import time
import random
import re
import sqlite3
import numpy as np 
import itertools
from collections import Counter
import matplotlib.pyplot as plt
import pickle

# --- C·∫§U H√åNH TRANG ---
st.set_page_config(page_title="Lottery AI V10.0 - Ensemble Models", layout="wide", page_icon="üß†")

# --- TH∆Ø VI·ªÜN AI N√ÇNG CAO ---
try:
    from sklearn.cluster import KMeans
    from sklearn.linear_model import LinearRegression, LogisticRegression
    from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
    from sklearn.neural_network import MLPRegressor
    from sklearn.preprocessing import MinMaxScaler
    from sklearn.model_selection import train_test_split
    import xgboost as xgb
    SKLEARN_AVAILABLE = True
except ImportError as e:
    SKLEARN_AVAILABLE = False
    st.error(f"‚ö†Ô∏è Thi·∫øu th∆∞ vi·ªán AI: {e}")

# ==================================================================================
# CLASS: DATABASE
# ==================================================================================
class DBManager:
    def __init__(self, db_file="lottery.db"):
        self.conn = sqlite3.connect(db_file, check_same_thread=False)
        self.conn.execute('''
            CREATE TABLE IF NOT EXISTS lottery_results (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                date TEXT, province_code TEXT, numbers TEXT,
                UNIQUE(date, province_code))
        ''')
        self.conn.commit()

    def upsert(self, date, code, nums):
        try:
            self.conn.execute('INSERT OR REPLACE INTO lottery_results (date, province_code, numbers) VALUES (?, ?, ?)', 
                             (date, code, ",".join(nums)))
            self.conn.commit()
            return True
        except: return False

    def get_df(self, code):
        df = pd.read_sql_query("SELECT date, numbers FROM lottery_results WHERE province_code = ? ORDER BY date DESC", self.conn, params=(code,))
        if df.empty: return None
        data = []
        for _, r in df.iterrows():
            row = {'Ngay': r['date']}
            for i, n in enumerate(r['numbers'].split(',')): row[f'Giai_{i}'] = n
            data.append(row)
        return pd.DataFrame(data)

# ==================================================================================
# CLASS: FEATURE ENGINEERING (T·∫†O ƒê·∫∂C TR∆ØNG CHO AI)
# ==================================================================================
class FeatureEngine:
    def __init__(self, df):
        self.df = df.copy()
        if not self.df.empty:
            try:
                self.df['DateObj'] = pd.to_datetime(self.df['Ngay'], dayfirst=True)
                self.df = self.df.sort_values(by='DateObj', ascending=True)
            except: pass
        
        self.history = []
        data_cols = [c for c in self.df.columns if c != 'Ngay']
        for _, row in self.df.iterrows():
            day_nums = []
            for col in data_cols:
                val = str(row[col])
                clean = ''.join(filter(str.isdigit, val))
                if len(clean) >= 2: day_nums.append(clean[-2:])
            self.history.append(day_nums)

    def extract_features(self, target_num, history_slice):
        """
        Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng c·ªßa 1 con s·ªë t·∫°i th·ªùi ƒëi·ªÉm c·ª• th·ªÉ d·ª±a tr√™n l·ªãch s·ª≠ tr∆∞·ªõc ƒë√≥.
        Features: Freq, Gan, AutoCorr, Last_Appear_Distance, v.v.
        """
        # 1. Frequency (trong 30 k·ª≥ g·∫ßn nh·∫•t c·ªßa slice)
        recent_30 = history_slice[-30:]
        flat_30 = [n for sub in recent_30 for n in sub]
        freq = flat_30.count(target_num)
        
        # 2. Gan (Kho·∫£ng c√°ch ch∆∞a v·ªÅ)
        gan = 0
        for sub in reversed(history_slice):
            if target_num in sub: break
            gan += 1
            
        # 3. AutoCorrelation (ƒê∆°n gi·∫£n h√≥a)
        series = [1 if target_num in sub else 0 for sub in history_slice[-50:]]
        if len(series) > 10 and np.var(series) > 0:
            # Lag 1 autocorrelation
            ac = pd.Series(series).autocorr(lag=1)
            ac = 0 if np.isnan(ac) else ac
        else:
            ac = 0
            
        return [freq, gan, ac]

    def create_training_dataset(self, lookback_days=100):
        """
        T·∫°o d·ªØ li·ªáu hu·∫•n luy·ªán: 
        X = C√°c ƒë·∫∑c tr∆∞ng ng√†y h√¥m qua
        y = K·∫øt qu·∫£ ng√†y h√¥m nay (1: v·ªÅ, 0: kh√¥ng v·ªÅ)
        """
        X = []
        y = []
        
        # Ch·ªâ l·∫•y 100 k·ª≥ g·∫ßn nh·∫•t ƒë·ªÉ train cho nhanh
        available_hist = self.history
        if len(available_hist) < 50: return None, None
        
        start_idx = len(available_hist) - lookback_days if len(available_hist) > lookback_days else 50
        
        for i in range(start_idx, len(available_hist)):
            past_data = available_hist[:i] # D·ªØ li·ªáu qu√° kh·ª© t√≠nh ƒë·∫øn ng√†y i-1
            current_result = available_hist[i] # K·∫øt qu·∫£ th·ª±c t·∫ø ng√†y i
            
            # L·∫•y m·∫´u ng·∫´u nhi√™n 10 s·ªë ƒë·ªÉ t·∫°o data train (tr√°nh qu√° t·∫£i)
            # Bao g·ªìm c·∫£ s·ªë v·ªÅ v√† s·ªë kh√¥ng v·ªÅ
            sample_nums = set(current_result) # Positive samples
            while len(sample_nums) < 20: # Th√™m Negative samples
                sample_nums.add(str(random.randint(0,99)).zfill(2))
            
            for num in sample_nums:
                features = self.extract_features(num, past_data)
                label = 1 if num in current_result else 0
                X.append(features)
                y.append(label)
                
        return np.array(X), np.array(y)

    def get_current_features(self):
        """L·∫•y ƒë·∫∑c tr∆∞ng ng√†y m·ªõi nh·∫•t ƒë·ªÉ d·ª± ƒëo√°n"""
        X_pred = []
        nums_map = []
        for i in range(100):
            num = str(i).zfill(2)
            feat = self.extract_features(num, self.history)
            X_pred.append(feat)
            nums_map.append(num)
        return np.array(X_pred), nums_map

# ==================================================================================
# CLASS: MODEL MANAGER (QU·∫¢N L√ù M√î H√åNH)
# ==================================================================================
class ModelManager:
    def __init__(self):
        self.models = {}
        self.scalers = {}

    def train_models(self, X, y, selected_models):
        if len(X) == 0: return
        
        # Scale d·ªØ li·ªáu
        scaler = MinMaxScaler()
        X_scaled = scaler.fit_transform(X)
        self.scalers['main'] = scaler
        
        # 1. Random Forest
        if 'Random Forest' in selected_models:
            rf = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)
            rf.fit(X_scaled, y)
            self.models['RF'] = rf
            
        # 2. Gradient Boosting (XGBoost)
        if 'XGBoost' in selected_models:
            xgb_model = xgb.XGBRegressor(n_estimators=100, learning_rate=0.05, max_depth=5, random_state=42)
            xgb_model.fit(X_scaled, y)
            self.models['XGB'] = xgb_model
            
        # 3. Neural Network (MLP)
        if 'Neural Network (MLP)' in selected_models:
            mlp = MLPRegressor(hidden_layer_sizes=(64, 32), activation='relu', solver='adam', max_iter=500, random_state=42)
            mlp.fit(X_scaled, y)
            self.models['MLP'] = mlp
            
        # 4. Linear Regression (Base)
        if 'Linear Regression' in selected_models:
            lr = LinearRegression()
            lr.fit(X_scaled, y)
            self.models['LR'] = lr

    def predict_ensemble(self, X_pred):
        if not self.models: return np.zeros(len(X_pred))
        
        scaler = self.scalers.get('main')
        if scaler:
            X_scaled = scaler.transform(X_pred)
        else:
            X_scaled = X_pred
            
        final_pred = np.zeros(len(X_pred))
        count = 0
        
        for name, model in self.models.items():
            pred = model.predict(X_scaled)
            final_pred += pred
            count += 1
            
        return final_pred / count if count > 0 else final_pred

# ==================================================================================
# UI & LOGIC CH√çNH
# ==================================================================================
# ... (Gi·ªØ nguy√™n c√°c h√†m scrape, parse HTML c≈©) ...
def get_nums_from_html(html, is_mb):
    soup = BeautifulSoup(html, 'html.parser')
    containers = soup.find_all('table', class_=re.compile(r'result|table|kqxs'))
    for tbl in containers:
        cells = tbl.find_all(['td', 'span'])
        nums = []
        for c in cells:
            txt = c.get_text().strip()
            if any(x in txt.lower() for x in ['gi·∫£i', 'ƒëb', 'ng√†y']): continue
            found = re.findall(r'\b\d{2,6}\b', txt)
            nums.extend(found)
        expected = 27 if is_mb else 18
        if len(nums) >= expected: return nums
    return []

def scrape_data(code, days):
    db = DBManager()
    count = 0
    now = datetime.now()
    progress = st.progress(0)
    for i in range(days):
        d = now - timedelta(days=i)
        ds = d.strftime("%d-%m-%Y"); ds_disp = d.strftime("%d/%m/%Y")
        url = f"https://xoso.com.vn/{code}-{ds}.html"
        try:
            res = requests.get(url, timeout=3)
            if res.status_code == 200:
                nums = get_nums_from_html(res.text, code=='xsmb')
                if nums:
                    check = [n for n in nums if n not in [d.strftime("%d"), d.strftime("%m")]]
                    expected = 27 if code=='xsmb' else 18
                    if len(check) >= expected:
                        db.upsert(ds_disp, code, check[:expected])
                        count += 1
        except: pass
        progress.progress((i+1)/days)
    st.success(f"ƒê√£ c·∫≠p nh·∫≠t {count} k·ª≥.")

# --- CACHING MODEL TRAINING ---
@st.cache_resource
def train_ai_manager(df_json, selected_models):
    # df_json l√† trick ƒë·ªÉ cache dataframe (hashable)
    df = pd.read_json(df_json)
    fe = FeatureEngine(df)
    X, y = fe.create_training_dataset(lookback_days=200)
    
    manager = ModelManager()
    if X is not None:
        manager.train_models(X, y, selected_models)
    
    return manager, fe

# --- GIAO DI·ªÜN ---
st.title("üß† Lottery AI V10.0 - Neural & Ensemble")

with st.sidebar:
    st.header("1. D·ªØ li·ªáu")
    PROVINCES = {"Mi·ªÅn B·∫Øc": "xsmb", "TP.HCM": "xshcm", "ƒê·ªìng Nai": "xsdn", "ƒê√† N·∫µng": "xsdng"}
    prov_name = st.selectbox("Ch·ªçn ƒê√†i", list(PROVINCES.keys()))
    prov_code = PROVINCES[prov_name]
    if st.button("C·∫≠p nh·∫≠t Data"): scrape_data(prov_code, 30)
    
    st.divider()
    st.header("2. M√¥ h√¨nh AI")
    models_opt = st.multiselect(
        "Ch·ªçn thu·∫≠t to√°n tham gia d·ª± ƒëo√°n:",
        ["Random Forest", "XGBoost", "Neural Network (MLP)", "Linear Regression"],
        default=["Random Forest", "XGBoost"]
    )
    
    st.info("üí° M·∫πo: Random Forest & XGBoost th∆∞·ªùng cho k·∫øt qu·∫£ t·ªët nh·∫•t v·ªõi d·ªØ li·ªáu d·∫°ng b·∫£ng.")
    
    btn_run = st.button("üöÄ K√çCH HO·∫†T AI", type="primary")

if btn_run:
    db = DBManager()
    df = db.get_df(prov_code)
    
    if df is not None:
        st.write(f"ƒêang hu·∫•n luy·ªán m√¥ h√¨nh tr√™n {len(df)} k·ª≥ quay... (Ti·∫øn tr√¨nh n√†y ƒë∆∞·ª£c Cache)")
        
        # Train & Cache Models
        manager, fe = train_ai_manager(df.to_json(), models_opt)
        
        # Predict Today
        X_pred, nums_map = fe.get_current_features()
        scores = manager.predict_ensemble(X_pred)
        
        # Create Result DataFrame
        res_df = pd.DataFrame({'so': nums_map, 'ai_score': scores})
        
        # Combine with basic stats
        all_nums = [n for day in fe.history for n in day]
        freq = pd.Series(all_nums).value_counts().reset_index()
        freq.columns = ['so', 'freq']
        res_df = res_df.merge(freq, on='so', how='left').fillna(0)
        
        # Calculate Final Score (Hybrid: AI + Traditional Stats)
        # AI score th∆∞·ªùng t·ª´ 0-1 (ho·∫∑c th·∫•p h∆°n), c·∫ßn scale l√™n
        res_df['final_score'] = (res_df['ai_score'] * 70) + (res_df['freq']/res_df['freq'].max() * 30)
        res_df = res_df.sort_values(by='final_score', ascending=False)
        
        # --- DISPLAY ---
        col1, col2 = st.columns([2, 1])
        
        with col1:
            st.subheader("üèÜ D·ª± ƒëo√°n c·ªßa AI Ensemble")
            st.dataframe(res_df.head(20).style.background_gradient(subset=['final_score'], cmap='Greens'), use_container_width=True)
            
        with col2:
            st.subheader("üìä ƒê·ªô tin c·∫≠y m√¥ h√¨nh")
            st.write("C√°c m√¥ h√¨nh ƒëang ch·∫°y:")
            for m in manager.models:
                st.write(f"- ‚úÖ {m}")
            
            # Simple Chart
            fig, ax = plt.subplots()
            top_10 = res_df.head(10)
            ax.bar(top_10['so'], top_10['final_score'], color='purple')
            st.pyplot(fig)
            
        # --- APRIORI (C·∫∂P S·ªê) ---
        st.divider()
        st.subheader("üîó Ph√¢n t√≠ch C·∫∑p S·ªë (Association Rules)")
        # Logic ƒë∆°n gi·∫£n h√≥a cho Streamlit
        pair_counts = Counter()
        for day in fe.history[-100:]:
            u = sorted(list(set(day)))
            for p in itertools.combinations(u, 2):
                pair_counts[f"{p[0]}-{p[1]}"] += 1
        
        top_pairs = pair_counts.most_common(10)
        cols = st.columns(5)
        for i, (p, c) in enumerate(top_pairs):
            cols[i%5].metric(label=f"C·∫∑p {p}", value=f"{c} l·∫ßn")

    else:
        st.error("Ch∆∞a c√≥ d·ªØ li·ªáu.")
